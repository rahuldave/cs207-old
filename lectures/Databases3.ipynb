{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://dl.dropboxusercontent.com/u/75194/Databases3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Project post milestone 2\n",
    "\n",
    "- Milestone 2 is due at 11.59PM two mondays from today. Try and finish it earlier though, as you will be hurting your post-milestone project if you dont.\n",
    "- additional things in milestone 2 will be to implement stored procedures (released wednesday) and predicate selects with orderable indices(released friday) \n",
    "- also make sure that writes to our database are atomic\n",
    "\n",
    "For you project after milestone 2, you must:\n",
    "- you must make this database persistent. You can choose a persistence of your choice, but one idea is a primary-key clustered index for the primary-key and rows, and bitmask indexes for low-cardinality \"factor\" or \"enum\" types which only take certain values and/or binary tree for the numeric ones (you will need to implement low-hi iteration) and/or repeated values for a key . Make sure changes remain atomic, and that they are durable.\n",
    "- represent vantage points in our persistent database. (this requires writing stored procedures for vantage point calculation and storing distance metadata, and then using the predicate selects. The algorithm is the one Pavlos described in class: choose vantage points proportional to a SAX word based representation. We will talk about this Friday, and do some part of an in-memory implementation).\n",
    "- there should be a http based REST api to the database. The API design is your choice, and depending on if you decided to write a synchronous client or an asynchronous client, you might want to choose flask or tornado/aiohttp.\n",
    "\n",
    "This much is compulsory. Additionally, each group can choose to do **one additional feature** of their choice, but let us know what this is by next monday and we can discuss it next monday (no lecture, just a lab, atleast one person from your group should come, but not all need to)\n",
    "\n",
    "Possible features are (your choice needs to be atleast as hard as this):\n",
    "\n",
    "1. a really fast FFT using cython on fftw integrated in\n",
    "2. vantage point storage using a vptree integrated in\n",
    "3. A multiprocessing based server where multiple processes write to the database. Implement atleast read-committed isolation.\n",
    "4. implement a proper language with repl as a client for the database\n",
    "5. your choice (we'll ask the TFs and Pavlos for more ideas if you like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions\n",
    "\n",
    "(most diagrams and all quotes are from Oreilly's Designing Data Intensive Applications: a book you should read if you, like me, enjoynthis stuff).\n",
    "\n",
    " •\tThe batch of operations is viewed as a single atomic operation, so all of the operations either succeed together or fail together.\n",
    " •\tThe database is in a valid state before and after the transaction.\n",
    " •\tThe batch update appears to be isolated; other queries should never see a database state in which only some of the operations have been applied.\n",
    " \n",
    "Databases have a mechanism for wrapping a single or multiple processes into a **Transaction**. This means that the batch of operations either all happen (**commit**) or not happen at all (**abort**, **rollback**). This is called **atomicity**.\n",
    "\n",
    "The NOSQL movement made transactions into a casualty since they were distributed and worried about replication and partitioning. The notion emerged that distributed-transactions could not be performant. But indeed this notion has been there from even earlier, as we shall see: different guarantee levels in transactions have different performance, even on a single machine, which is the case we focus on in this class.\n",
    "\n",
    "The transactional guarantees are represented by the acronym `ACID`. The A is for atomicity, which we just described.\n",
    "\n",
    "* C is for **Consistency**: data invariants must be true. This is really a property of the application: eg accounting tables must be balanced. Databases can help with foreign keys, but this is a property of the app. We wont discuss this one further,\n",
    "* D is for **Durability**: once a transaction has comitted successfully, data comitted wont be forgotten. This requires persistent storage, or replication, or both\n",
    "* I is for **Isolation**. This is the most interesting of the lot, and critical to the sensible running of a databse. The idea is that transactions should not step on each other. Each transaction should pretends that its the only one running on the databse: in other words, as if the transactions wer completely serialized. In practice this would make things very slow, so we try different transactional guarantees that fall short of explicit serialization except in the situations that really need serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atomicity and Isolation are usually provided by most databases for single object writes, like a single set (or for that matter a single get) on one machine. As we shall see later, these are critical to prevent \"lost updates\" and are a big part of the transaction story, one that is implemented even in the new-fangled databases. But they are not the full story. We need multi-object transactionsto\n",
    "\n",
    "- deal with foreign keys\n",
    "- deal with the denormalization seen in nosql's like mongo\n",
    "- deal with secondary indexes.\n",
    "\n",
    "Lets look at all of these scenarios using two examples: a relational setup, and our append based binary-tree key-value database or index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueRef(object):\n",
    "    \" a reference to a string value on disk\"\n",
    "    def __init__(self, referent=None, address=0):\n",
    "        self._referent = referent #value to store\n",
    "        self._address = address #address to store at\n",
    "        \n",
    "    @property\n",
    "    def address(self):\n",
    "        return self._address\n",
    "    \n",
    "    def prepare_to_store(self, storage):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def referent_to_bytes(referent):\n",
    "        return referent.encode('utf-8')\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_referent(bytes):\n",
    "        return bytes.decode('utf-8')\n",
    "\n",
    "    \n",
    "    def get(self, storage):\n",
    "        \"read bytes for value from disk\"\n",
    "        if self._referent is None and self._address:\n",
    "            self._referent = self.bytes_to_referent(storage.read(self._address))\n",
    "        return self._referent\n",
    "\n",
    "    def store(self, storage):\n",
    "        \"store bytes for value to disk\"\n",
    "        #called by BinaryNode.store_refs\n",
    "        if self._referent is not None and not self._address:\n",
    "            self.prepare_to_store(storage)\n",
    "            self._address = storage.write(self.referent_to_bytes(self._referent))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class BinaryNodeRef(ValueRef):\n",
    "    \"reference to a btree node on disk\"\n",
    "    \n",
    "    #calls the BinaryNode's store_refs\n",
    "    def prepare_to_store(self, storage):\n",
    "        \"have a node store its refs\"\n",
    "        if self._referent:\n",
    "            self._referent.store_refs(storage)\n",
    "\n",
    "    @staticmethod\n",
    "    def referent_to_bytes(referent):\n",
    "        \"use pickle to convert node to bytes\"\n",
    "        return pickle.dumps({\n",
    "            'left': referent.left_ref.address,\n",
    "            'key': referent.key,\n",
    "            'value': referent.value_ref.address,\n",
    "            'right': referent.right_ref.address,\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_referent(string):\n",
    "        \"unpickle bytes to get a node object\"\n",
    "        d = pickle.loads(string)\n",
    "        return BinaryNode(\n",
    "            BinaryNodeRef(address=d['left']),\n",
    "            d['key'],\n",
    "            ValueRef(address=d['value']),\n",
    "            BinaryNodeRef(address=d['right']),\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryNode(object):\n",
    "    @classmethod\n",
    "    def from_node(cls, node, **kwargs):\n",
    "        \"clone a node with some changes from another one\"\n",
    "        return cls(\n",
    "            left_ref=kwargs.get('left_ref', node.left_ref),\n",
    "            key=kwargs.get('key', node.key),\n",
    "            value_ref=kwargs.get('value_ref', node.value_ref),\n",
    "            right_ref=kwargs.get('right_ref', node.right_ref),\n",
    "        )\n",
    "\n",
    "    def __init__(self, left_ref, key, value_ref, right_ref):\n",
    "        self.left_ref = left_ref\n",
    "        self.key = key\n",
    "        self.value_ref = value_ref\n",
    "        self.right_ref = right_ref\n",
    "\n",
    "    def store_refs(self, storage):\n",
    "        \"method for a node to store all of its stuff\"\n",
    "        self.value_ref.store(storage)\n",
    "        #calls BinaryNodeRef.store. which calls\n",
    "        #BinaryNodeRef.prepate_to_store\n",
    "        #which calls this again and recursively stores\n",
    "        #the whole tree\n",
    "        self.left_ref.store(storage)\n",
    "        self.right_ref.store(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryTree(object):\n",
    "    \"Immutable Binary Tree class. Constructs new tree on changes\"\n",
    "    def __init__(self, storage):\n",
    "        self._storage = storage\n",
    "        self._refresh_tree_ref()\n",
    "\n",
    "    def commit(self):\n",
    "        \"changes are final only when committed\"\n",
    "        #triggers BinaryNodeRef.store\n",
    "        self._tree_ref.store(self._storage)\n",
    "        #make sure address of new tree is stored\n",
    "        self._storage.commit_root_address(self._tree_ref.address)\n",
    "\n",
    "    def _refresh_tree_ref(self):\n",
    "        \"get reference to new tree if it has changed\"\n",
    "        self._tree_ref = BinaryNodeRef(\n",
    "            address=self._storage.get_root_address())\n",
    "\n",
    "    def get(self, key):\n",
    "        \"get value for a key\"\n",
    "        #your code here\n",
    "        #if tree is not locked by another writer\n",
    "        #refresh the references and get new tree if needed\n",
    "        if not self._storage.locked:\n",
    "            self._refresh_tree_ref()\n",
    "        #get the top level node\n",
    "        node = self._follow(self._tree_ref)\n",
    "        #traverse until you find appropriate node\n",
    "        while node is not None:\n",
    "            if key < node.key:\n",
    "                node = self._follow(node.left_ref)\n",
    "            elif key < node.key:\n",
    "                node = self._follow(node.right_ref)\n",
    "            else:\n",
    "                return self._follow(node.value_ref)\n",
    "        raise KeyError\n",
    "\n",
    "    def set(self, key, value):\n",
    "        \"set a new value in the tree. will cause a new tree\"\n",
    "        #try to lock the tree. If we succeed make sure\n",
    "        #we dont lose updates from any other process\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        #get current top-level node and make a value-ref\n",
    "        node = self._follow(self._tree_ref)\n",
    "        value_ref = ValueRef(value)\n",
    "        #insert and get new tree ref\n",
    "        self._tree_ref = self._insert(node, key, value_ref)\n",
    "        \n",
    "    \n",
    "    def _insert(self, node, key, value_ref):\n",
    "        \"insert a new node creating a new path from root\"\n",
    "        #create a tree ifnthere was none so far\n",
    "        if node is None:\n",
    "            new_node = BinaryNode(\n",
    "                BinaryNodeRef(), key, value_ref, BinaryNodeRef())\n",
    "        elif key < node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                left_ref=self._insert(\n",
    "                    self._follow(node.left_ref), key, value_ref))\n",
    "        elif key > node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                right_ref=self._insert(\n",
    "                    self._follow(node.right_ref), key, value_ref))\n",
    "        else: #create a new node to represent this data\n",
    "            new_node = BinaryNode.from_node(node, value_ref=value_ref)\n",
    "        return BinaryNodeRef(referent=new_node)\n",
    "\n",
    "    def delete(self, key):\n",
    "        \"delete node with key, creating new tree and path\"\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        node = self._follow(self._tree_ref)\n",
    "        self._tree_ref = self._delete(node, key)\n",
    "        \n",
    "    def _delete(self, node, key):\n",
    "        \"underlying delete implementation\"\n",
    "        if node is None:\n",
    "            raise KeyError\n",
    "        elif key < node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                left_ref=self._delete(\n",
    "                    self._follow(node.left_ref), key))\n",
    "        elif key > node.key:\n",
    "            new_node = BinaryNode.from_node(\n",
    "                node,\n",
    "                right_ref=self._delete(\n",
    "                    self._follow(node.right_ref), key))\n",
    "        else:\n",
    "            left = self._follow(node.left_ref)\n",
    "            right = self._follow(node.right_ref)\n",
    "            if left and right:\n",
    "                replacement = self._find_max(left)\n",
    "                left_ref = self._delete(\n",
    "                    self._follow(node.left_ref), replacement.key)\n",
    "                new_node = BinaryNode(\n",
    "                    left_ref,\n",
    "                    replacement.key,\n",
    "                    replacement.value_ref,\n",
    "                    node.right_ref,\n",
    "                )\n",
    "            elif left:\n",
    "                return node.left_ref\n",
    "            else:\n",
    "                return node.right_ref\n",
    "        return BinaryNodeRef(referent=new_node)\n",
    "\n",
    "    def _follow(self, ref):\n",
    "        \"get a node from a reference\"\n",
    "        #calls BinaryNodeRef.get\n",
    "        return ref.get(self._storage)\n",
    "    \n",
    "    def _find_max(self, node):\n",
    "        while True:\n",
    "            next_node = self._follow(node.right_ref)\n",
    "            if next_node is None:\n",
    "                return node\n",
    "            node = next_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "\n",
    "import portalocker\n",
    "\n",
    "\n",
    "class Storage(object):\n",
    "    SUPERBLOCK_SIZE = 4096\n",
    "    INTEGER_FORMAT = \"!Q\"\n",
    "    INTEGER_LENGTH = 8\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self._f = f\n",
    "        self.locked = False\n",
    "        #we ensure that we start in a sector boundary\n",
    "        self._ensure_superblock()\n",
    "\n",
    "    def _ensure_superblock(self):\n",
    "        \"guarantee that the next write will start on a sector boundary\"\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        end_address = self._f.tell()\n",
    "        if end_address < self.SUPERBLOCK_SIZE:\n",
    "            self._f.write(b'\\x00' * (self.SUPERBLOCK_SIZE - end_address))\n",
    "        self.unlock()\n",
    "\n",
    "    def lock(self):\n",
    "        \"if not locked, lock the file for writing\"\n",
    "        if not self.locked:\n",
    "            portalocker.lock(self._f, portalocker.LOCK_EX)\n",
    "            self.locked = True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def unlock(self):\n",
    "        if self.locked:\n",
    "            self._f.flush()\n",
    "            portalocker.unlock(self._f)\n",
    "            self.locked = False\n",
    "\n",
    "    def _seek_end(self):\n",
    "        self._f.seek(0, os.SEEK_END)\n",
    "\n",
    "    def _seek_superblock(self):\n",
    "        \"go to beginning of file which is on sec boundary\"\n",
    "        self._f.seek(0)\n",
    "\n",
    "    def _bytes_to_integer(self, integer_bytes):\n",
    "        return struct.unpack(self.INTEGER_FORMAT, integer_bytes)[0]\n",
    "\n",
    "    def _integer_to_bytes(self, integer):\n",
    "        return struct.pack(self.INTEGER_FORMAT, integer)\n",
    "\n",
    "    def _read_integer(self):\n",
    "        return self._bytes_to_integer(self._f.read(self.INTEGER_LENGTH))\n",
    "\n",
    "    def _write_integer(self, integer):\n",
    "        self.lock()\n",
    "        self._f.write(self._integer_to_bytes(integer))\n",
    "\n",
    "    def write(self, data):\n",
    "        \"write data to disk, returning the adress at which you wrote it\"\n",
    "        #first lock, get to end, get address to return, write size\n",
    "        #write data, unlock <==WRONG, dont want to unlock here\n",
    "        #your code here\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        object_address = self._f.tell()\n",
    "        self._write_integer(len(data))\n",
    "        self._f.write(data)\n",
    "        return object_address\n",
    "\n",
    "    def read(self, address):\n",
    "        self._f.seek(address)\n",
    "        length = self._read_integer()\n",
    "        data = self._f.read(length)\n",
    "        return data\n",
    "\n",
    "    def commit_root_address(self, root_address):\n",
    "        self.lock()\n",
    "        self._f.flush()\n",
    "        #make sure you write root address at position 0\n",
    "        self._seek_superblock()\n",
    "        #write is atomic because we store the address on a sector boundary.\n",
    "        self._write_integer(root_address)\n",
    "        self._f.flush()\n",
    "        self.unlock()\n",
    "\n",
    "    def get_root_address(self):\n",
    "        #read the first integer in the file\n",
    "        #your code here\n",
    "        self._seek_superblock()\n",
    "        root_address = self._read_integer()\n",
    "        return root_address\n",
    "\n",
    "    def close(self):\n",
    "        self.unlock()\n",
    "        self._f.close()\n",
    "\n",
    "    @property\n",
    "    def closed(self):\n",
    "        return self._f.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBDB(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self._storage = Storage(f)\n",
    "        self._tree = BinaryTree(self._storage)\n",
    "\n",
    "    def _assert_not_closed(self):\n",
    "        if self._storage.closed:\n",
    "            raise ValueError('Database closed.')\n",
    "\n",
    "    def close(self):\n",
    "        self._storage.close()\n",
    "\n",
    "    def commit(self):\n",
    "        self._assert_not_closed()\n",
    "        self._tree.commit()\n",
    "\n",
    "    def get(self, key):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.get(key)\n",
    "\n",
    "    def set(self, key, value):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.set(key, value)\n",
    "\n",
    "    def delete(self, key):\n",
    "        self._assert_not_closed()\n",
    "        return self._tree.delete(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect(dbname):\n",
    "    try:\n",
    "        f = open(dbname, 'r+b')\n",
    "    except IOError:\n",
    "        fd = os.open(dbname, os.O_RDWR | os.O_CREAT)\n",
    "        f = os.fdopen(fd, 'r+b')\n",
    "    return DBDB(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm /tmp/test2.dbdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "db = connect(\"/tmp/test2.dbdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-31905ff9ed7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/test2.dbdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rahul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-d0e8d3b6ca15>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-048548199505>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_follow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"rahul\", \"young\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"rahul\", \"young\")\n",
    "db.commit()\n",
    "db.close()\n",
    "db = connect(\"/tmp/test2.dbdb\")\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.delete(\"pavlos\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical parts of the code from above are these:\n",
    "\n",
    "from Storage:\n",
    "\n",
    "```python\n",
    "    def commit_root_address(self, root_address):\n",
    "        self.lock()\n",
    "        self._f.flush()\n",
    "        #make sure you write root address at position 0\n",
    "        self._seek_superblock()\n",
    "        #write is atomic because we store the address on a sector boundary.\n",
    "        self._write_integer(root_address)\n",
    "        self._f.flush()\n",
    "        self.unlock()\n",
    "        \n",
    "    def _ensure_superblock(self):\n",
    "        \"guarantee that the next write will start on a sector boundary\"\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        end_address = self._f.tell()\n",
    "        if end_address < self.SUPERBLOCK_SIZE:\n",
    "            self._f.write(b'\\x00' * (self.SUPERBLOCK_SIZE - end_address))\n",
    "        self.unlock()\n",
    "```\n",
    "\n",
    "get and set on the BinaryTree:\n",
    "\n",
    "```python\n",
    "    def get(self, key):\n",
    "        \"get value for a key\"\n",
    "        #your code here\n",
    "        #if tree is not locked by another writer\n",
    "        #refresh the references and get new tree if needed\n",
    "        if not self._storage.locked:\n",
    "            self._refresh_tree_ref()\n",
    "        #get the top level node\n",
    "        node = self._follow(self._tree_ref)\n",
    "        #traverse until you find appropriate node\n",
    "        while node is not None:\n",
    "            if key < node.key:\n",
    "                node = self._follow(node.left_ref)\n",
    "            elif key < node.key:\n",
    "                node = self._follow(node.right_ref)\n",
    "            else:\n",
    "                return self._follow(node.value_ref)\n",
    "        raise KeyError\n",
    "\n",
    "    def set(self, key, value):\n",
    "        \"set a new value in the tree. will cause a new tree\"\n",
    "        #try to lock the tree. If we succeed make sure\n",
    "        #we dont lose updates from any other process\n",
    "        if self._storage.lock():\n",
    "            self._refresh_tree_ref()\n",
    "        #get current top-level node and make a value-ref\n",
    "        node = self._follow(self._tree_ref)\n",
    "        value_ref = ValueRef(value)\n",
    "        #insert and get new tree ref\n",
    "        self._tree_ref = self._insert(node, key, value_ref)\n",
    "        \n",
    "    def _write_integer(self, integer):\n",
    "        self.lock()\n",
    "        self._f.write(self._integer_to_bytes(integer))\n",
    "\n",
    "    def write(self, data):\n",
    "        \"write data to disk, returning the adress at which you wrote it\"\n",
    "        #first lock, get to end, get address to return, write size\n",
    "        #write data, unlock\n",
    "        #your code here\n",
    "        self.lock()\n",
    "        self._seek_end()\n",
    "        object_address = self._f.tell()\n",
    "        self._write_integer(len(data))\n",
    "        self._f.write(data)\n",
    "        return object_address\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out the address of the root node ia atomic since we are on a disk sector boundary, and this is the very first thing in the file. The disk-hardware guarantees that single-sector writes are atomic. \n",
    "\n",
    "The above case is atomic in the single-object sense because the changes are made and comitted, creating a new version of the tree. If things fail, the tree wont be created and you are back to the previous version. We havent explicitly built in any abort-handling though, replying on the python exceptions to do this for us. Another process will NEVER see a mix of writes: the root address will either be the old value or the new one. \n",
    "\n",
    "Our very own in memory process, will, however, reflect the partial changes we made, so a proper transaction manager should kill the \"thread\" of execution. But we havent talked about a process model yet, and will come to this the next time. Today we'll assume we have different processes (so that we dont have to worry about the GIL in thinking about any of this).\n",
    "\n",
    "We also write the new data (the new tree) to disk (and flush) before we update the root address, there is no way to get at them as yet. When the root address is updated, we are guaranteed its data is now on disk. This ensures durability: once the transaction is comitted, its dats are on disk, and until it is comitted, this data is not reachable, indeed only the old data is (even if the new data is eating up pprecious bytes).\n",
    "\n",
    "But what about isolation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is isolation important?\n",
    "\n",
    "It is hard to program without isolation. isolation is what guarantees stability. It is what makes sure that there are no dirty reads and dirty writes.\n",
    "\n",
    "From designing data intensive applications:\n",
    "\n",
    "Dirty reads\n",
    "\n",
    ">One client reads another client’s writes before they have been committed. The read committed isolation level and stronger levels prevent dirty reads.\n",
    "\n",
    "\n",
    "Dirty writes\n",
    "\n",
    ">One client overwrites data that another client has written, but not yet committed. Almost all transaction implementations prevent dirty writes.\n",
    "\n",
    "Clearly, the notions of isolation are really the notions of concurrenvy: these issues will also occut when2 programs access any data, in mempry or in a database. In both cases locks and other ideas must be used to make sure that there is only one mutator at a time, and that an object is not exposed in an inconsistent state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Committed\n",
    "\n",
    "No dirty reads and no dirty writes.\n",
    "\n",
    "Dirty reads means that you can peek into the goings on inside a transaction and get at itermediate values. This could mean you see a value that would be later rolled back.\n",
    "\n",
    "with dirty writes you could be overwriting an uncomitted value\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/dirtywrite.png)\n",
    "\n",
    "In Read Comitted mode, in essence we need to remember the pre-transaction value, ie the old comitted value and the also the new value being set by the transaction which holds the current write lock. While this transaction runs, any other thread/transaction sees the old value. Once this one commits, then the values read by the other transaction are switched. And once the lock is held, no-one else can write.\n",
    "\n",
    "You can see this in the `get` above where we check whether any other transaction is running and if not get the latest tree. In a regular database we similarly use row-level locks.Another writer must wait until the lock is given up, and the reader will read the value before the lock was set as long as the write transaction is in progress.\n",
    "\n",
    "So if we implemented a transaction system for our database, the current code would put it in Read Committed mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Read Skew Problem\n",
    "\n",
    ">Non-repeatable reads:\n",
    ">A client sees different parts of the database at different points in time. This is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with multi-version concurrency control (MVCC).\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/readskew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the diagram above, it makes perfect sence if Alice's two reads are separate read transactions. But what if they are one transaction and we are in read-comitted mode. Since the reads took a very long time and were on both sides of a kosher transaction, they might give Alice a heart attack. This is a non-repeatable read as if Alice were to re-issue the reas transaction, acct 1 would now show 600. This problem is perfectly consistent with operations in read-comitted mode: the second read gets the switched value in analog to our get.\n",
    "\n",
    "The fix for this would seem to be clear: if both the reads happen in one transaction, one must make sure that because this transaction happened before the write, it sees the older version of the accounts (500, 500). Cast into the language of our database, the check we do for a get ought to be suspended if the get is inside an already started transaction. In general, the only place a tree-refresh ought to be allowed is at the beginning of a read-or write transaction.\n",
    "\n",
    "These thoughts form the basis of Multi-Version concurrency control, or MVCC, also known as snapshot isolation.\n",
    "\n",
    "The key principle is (from DDIA):\n",
    "> readers never block writers, and writers never block readers\n",
    "\n",
    "Here then are the rules:\n",
    "\n",
    ">1. At the start of each transaction, the database makes a list of all the other transac‐ tions which are in progress (not yet committed or aborted) at that time. Any writes made by one of those transactions are ignored, even if the transaction sub‐ sequently commits.\n",
    "2. Any writes made by aborted transactions are ignored.\n",
    "3. Any writes made by transactions with a later transaction ID (i.e. which started after the current transaction started) are ignored, regardless of whether that transaction has committed.\n",
    "4. All other writes are visible to the application’s queries.\n",
    "\n",
    "Here's what snapshot isolation using MVCC looks like:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/mvcc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about writes?\n",
    "\n",
    "There is another problem we encounter here: \n",
    "\n",
    "### the lost update problem.\n",
    "\n",
    "Consider an upsert, where we read a value, do something with it, like the balance change above, and then write it. Because we only lock writes, a transaction T1 might read a value V0, and then transaction T2 reads value V0. Since T2 does not have lock, the code in `get` wont get a new tree, and uses the old value. This is good as the transaction T1 is still running.\n",
    "\n",
    "But here is the problem: T1 will create a new tree, say with V0+1. Now T2 comes along, operates on V0, and makes it V0+2. say V0 was 5 to start with. The notion of the process might have been 5->6->8. But we wont do that. Our latest value will be 7.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/lostupdate.png)\n",
    "\n",
    "The fix to this is atomic updates/upserts. The `UPDATE` command in sql does just that, and if we build transactional facility into our database to do multiple gets, there is no reason not to extend it to an upsert. The entire upsert would grab the lock, and once that happens this would effectively serialize T1 and T2. Atomic updates are usually thus built by taking the lock when the object is read, or by forcing all atomic operations to be on one given thread.\n",
    "\n",
    "So thus it seems we can add a transaction manager, have an explicit notion of read, write, and upsert transactions, and have a reasonable snapshot isolated database.\n",
    "\n",
    "But even this does not solve some problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write skew\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/writeskew.png)\n",
    "\n",
    "Imagine the following situation: you and i both concurrently try and book the same time slot. you go first and book it. I start before you. and you start next. We both check to see if there is a booking at this time, and there is not. If each booking is a different object, then we both can make the booking: this correspondsto set on two separate keys which have an application level commonality in meaning but not necessarily at the database level. Snapshot isolation will happily allow this.\n",
    "\n",
    "Notice this is a multi object transaction with a select based on some predicate and stuff changes under the second select.\n",
    "\n",
    "This would only work if the transactions were fully serialized, as then the second bookers check would wait on the first bookers transaction being completed and thus raise the conflict. \n",
    "\n",
    "> This effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.\n",
    "\n",
    "One could solve this by having a timeslot-room key but now you are letting concurrency dictate your application model. The most general solution is to use serialized isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialized Isolation\n",
    "\n",
    "- one could actually run things in serial on one thread. With in memory databases this has become faster. Voltdb/h-store does this with replication, and stored procedures (in java and groovy, a JVM language).\n",
    "\n",
    "#### stored procedures\n",
    "\n",
    "Stored procedures run inside the database address space, the idea being to eliminate the overhead of serialization and de-serialization, network transfer, etc (but a badly written stored procedure could be worse)\n",
    "\n",
    "#### multithreaded\n",
    "\n",
    "but if you want multithreaded, the old, but not gold since its slowness makes people not like it is 2-phase locking(2PL). \n",
    "\n",
    "- the ingredients are shared read locks and exclusive write locks.\n",
    "\n",
    "The process then is:\n",
    "\n",
    ">- if a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, the transaction must wait.\n",
    "- If a transaction wants to write to an object, it must first acquire the lock in exclu‐ sive mode. No other transaction may hold the lock at the same time (neither in shared nor in exclusive mode), so if there is any existing lock on the object, the transaction must wait.\n",
    "- If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.\n",
    "- After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two- phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this in our database has the flavor of serialized upserts, except that we now even want to serialize the 2 separate sets. So then, reading the bookings, the get, would have a shared lock. If T1 has an exclusive lock to write in there, everyone else must wait on it to complete. At this point T2 cant even read until T1 is complete, so when T2 runs, it gets the latest info. Now T2 can write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicate and Range Locks\n",
    "\n",
    "Now to fix the room booking problem, we want to do  a predicate lock on the \"key\", here a set of time bins that we wish to occupy. You can imagine the conditional checking after pulling from the database is an expensive operation. Thus we can lock a range of keys instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
