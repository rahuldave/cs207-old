{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfaces, Implementations, and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of a program\n",
    "\n",
    "From the Practice of Programming:\n",
    "\n",
    ">The essence of design is to balance competing goals and constraints. Although there may be many tradeoffs when one is writing a small self-contained system, the ramifications of particular choices remain within the system and affect only the indi- vidual programmer. But when code is to be used by others, decisions have wider repercussions.\n",
    "\n",
    "\n",
    "What are the issues we need to be cognizant of?\n",
    "\n",
    "- **Interfaces**: your program is being designed to be used by someone: either an end user, another programmer (you are writing a library), or even yourself (we are talking about some layer in your program). This **interface** is a contract between you and the user: with preconditions, postconditions, etc\n",
    "- There is **information** hiding between layers (a higher up layer can be more abstract, more lossy about all the information). Encapsulation,  abstraction,  modularization, are some of the techniques used here.\n",
    "- There are **resource management** issues: who allocates storage for data structures.. (generally we want resource allocation/deallocation to happen in the same layer)\n",
    "- How to **deal with errors**: do we return special values, throw exceptions? who handles them? (generally we want to catch even the lowest level error and give the client the chance to handle it, possibly lossily)\n",
    "\n",
    "### Interface principles\n",
    "\n",
    "Interfaces should:\n",
    "\n",
    "- hide implementation details\n",
    "- have a small set of operations exposed, the smallest possible, and these should be orthogonal. Be stingy with the user.\n",
    "- but be transparent with the user in what goes on behind the scenes (calls the NSA)\n",
    "- be consistent internally: library functions should have similar signature, classes similar methods, and externally: programs should have the same cli flags, same ops.\n",
    "\n",
    "** Testing should deal with ALL of the issues above, and each layer ought to be tested separately **. \n",
    "\n",
    "This gives rise to :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different kinds of tests\n",
    "\n",
    "- **acceptance tests** verify that a program meets a customer's expectations. In a sense these are a test of the *interface* to the customer: does the program do everything you promised the customer it would do? You might use test harnesees for cli programs and selenium for this. The test of a library interface could also be thought of as an acceptance test\n",
    "\n",
    "- **unit tests** are tests which test a unit of the program, for use by another unit. These could test the interface for a client, but must also be testing internal functions which you want to use.\n",
    "\n",
    "Exploratory testing, regression testing, and integration testing are done in both of these categories, with the latter trying to combine layers and subsystems, not necessarily at the level of an entire application. \n",
    "\n",
    "One can also performance test, random and exploratorily test, and stress test a system (to create adversarial situations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing of a program\n",
    "\n",
    "Test as you write your program.\n",
    "\n",
    "This is so important that I repeat it.\n",
    "\n",
    "**Test as you go**.\n",
    "\n",
    "You will cry otherwise. I have in the past.\n",
    "\n",
    "From The Practice of Programming:\n",
    "\n",
    "\n",
    ">The  effort  of  testing as  you  go  is  minimal  and  pays off  handsomely.  Thinking about testing as you  write a program will  lead to better code, because that's when you know  best  what the code should do.  If  instead  you  wait  until  something breaks, you will  probably  have forgotten how  the code works.  Working under  pressure, you  will need  to figure it  out again, which  takes time, and  the fixes  will  be  less  thorough  and more fragile because your refreshed understanding is  likely to be incomplete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions and the process of testing\n",
    "\n",
    "The workhorse of testing is the `assert` statement. The same statement can also be used to assert preconditions and postconditions, and thus to test them if appropriate. In C, `assert` is a macro which can be conditionally compiled away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myaverage(l:list)->float:\n",
    "    sumit = 0.0\n",
    "    for f in l:\n",
    "        sumit = sumit + f\n",
    "    average = sumit/len(l)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert myaverage([1,2])==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-effec7f1a9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmyaverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert myaverage([1,2])==3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principles of testing\n",
    "\n",
    "#### Test Simple Parts First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_average():\n",
    "    assert myaverage([1,2])==1.5, \"1 and 2 must average to 1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myaverage(l:list)->float:\n",
    "    average = 0.0\n",
    "    for f in l:\n",
    "        average = average + 2*f\n",
    "    average = average/len(l)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "1 and 2 must average to 1.5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a9de5b052499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5ec8461ea141>\u001b[0m in \u001b[0;36mtest_average\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mmyaverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1 and 2 must average to 1.5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: 1 and 2 must average to 1.5"
     ]
    }
   ],
   "source": [
    "test_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def myaverage(l:list)->float:\n",
    "    n = len(l)\n",
    "    thesum = sum(l)\n",
    "    average = thesum/n\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test code at its boundaries\n",
    "\n",
    "The idea is that most errors happen at data boundaries such as empty input, single input item, exactly full array, wierd values, etc. If a piece of code works at the boundaries, its likely to work elsewhere...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d83504e73c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyaverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a4032b9131af>\u001b[0m in \u001b[0;36mmyaverage\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mthesum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthesum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "myaverage([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program defensively \n",
    "\n",
    "The user is supposed to give us an array, non-zero. We could specify that as a precondition, but we might as well be defensive and test for that.\n",
    "\n",
    "Practice:\n",
    ">\"Program  defensively.  A  useful  technique is  to  add  code  to  handle  \"can't  happen\" cases,  situations  where  it  is  not  logically  possible  for  something  to  happen  but (because of  some failure elsewhere) it might anyway.  Adding a test for zero or nega- tive array lengths to avg  was one example.  As another example, a program  process- ing  grades might  expect  that  there  would  be  no  negative  or huge  values  but  should check anyway: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From https://docs.python.org/3/library/exceptions.html:\n",
    " \n",
    " >exception **ValueError**\n",
    " \n",
    ">Raised when a built-in operation or function receives an argument that has the right type but an inappropriate value, and the situation is not described by a more precise exception such as IndexError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myaverage(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \"\"\"\n",
    "    n = len(l)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate mean of length 0 list\")\n",
    "    thesum = sum(l)\n",
    "    average = thesum/n\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi <class 'ValueError'> ('cant calculate mean of length 0 list',)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    myaverage([])\n",
    "except Exception as e:\n",
    "    print(\"hi\",type(e), e.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_average_empty():\n",
    "    try:\n",
    "        myaverage([])\n",
    "    except Exception as e:\n",
    "        assert (type(e) == ValueError and e.args[0]=='cant calculate mean of length 0 list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_average_empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automate using a test harness\n",
    "\n",
    "Doctests are one way to do this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymath.py\n"
     ]
    }
   ],
   "source": [
    "%%file mymath.py\n",
    "\n",
    "\n",
    "def myaverage(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> myaverage([1,2])\n",
    "    1.5\n",
    "    >>> myaverage([])\n",
    "    Traceback (most recent call last):\n",
    "        ...\n",
    "    ValueError: cant calculate mean of length 0 list\n",
    "    \"\"\"\n",
    "    n = len(l)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate mean of length 0 list\")\n",
    "    thesum = sum(l)\n",
    "    average = thesum/n\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\r\n",
      "    myaverage([1,2])\r\n",
      "Expecting:\r\n",
      "    1.5\r\n",
      "ok\r\n",
      "Trying:\r\n",
      "    myaverage([])\r\n",
      "Expecting:\r\n",
      "    Traceback (most recent call last):\r\n",
      "        ...\r\n",
      "    ValueError: cant calculate mean of length 0 list\r\n",
      "ok\r\n",
      "1 items had no tests:\r\n",
      "    mymath\r\n",
      "1 items passed all tests:\r\n",
      "   2 tests in mymath.myaverage\r\n",
      "2 tests in 2 items.\r\n",
      "2 passed and 0 failed.\r\n",
      "Test passed.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m doctest mymath.py --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unittest framework\n",
    "\n",
    "But too many doctests clutter the documentation of a class.\n",
    "\n",
    "One should only have those examples as doctests which describe the various ways a class or function can be used. Edge cases which must work, etc, ought to be represented in a separate test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_mymath.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath.py\n",
    "\n",
    "\n",
    "import unittest\n",
    "\n",
    "from mymath import myaverage\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_mymath(self):\n",
    "        self.assertEqual(myaverage([2,3]), 2.5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 test_mymath.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you get an error\n",
    "\n",
    "It could be that:\n",
    "\n",
    "- you messed up an implementation (we saw this above with the mult by two)\n",
    "- if the error was not found in an existing test, create a new test that represents the problem BEFORE you do anything else. The test shold capture the essence of the problem: this process itself is useful in uncovering bugs. Then this error may even suggest more tests. You fix, perhaps by writing defensive code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ca4d377dd12b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyaverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-67be5128c677>\u001b[0m in \u001b[0;36mmyaverage\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cant calculate mean of length 0 list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mthesum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthesum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "myaverage(['a',1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Incrementally\n",
    "\n",
    "In this way you test incrementally, adding tests all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_mymath(self):\n",
    "        self.assertEqual(myaverage([2,3]), 2.5)\n",
    "        \n",
    "    def test_char(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            myaverage(['a',3])\n",
    "            \n",
    "    def test_zerol(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            myaverage([])\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(MyTest())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pre-conditions and post-conditions\n",
    "\n",
    "While we could take the position that bad pre-conditions lead to undefined behavior, we are good citizens if we test preconditions for the user...and this helps during algorithm development as well. Here we test 2 pre-conditions: no zero length, and numerical array. The latter would be too expensive at the beginning, so we do it by wrapping `sum` in a `try..except` block. We could test postcondition by asserting that the average is a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myaverage(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> myaverage([1,2])\n",
    "    1.5\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(l)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate mean of length 0 list\")\n",
    "    try:\n",
    "        thesum = sum(l)\n",
    "    except:\n",
    "        raise TypeError(\"Cannot sum things of different types\")\n",
    "    average = thesum/n\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot sum things of different types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-fc8c436283a6>\u001b[0m in \u001b[0;36mmyaverage\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mthesum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ca4d377dd12b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyaverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-fc8c436283a6>\u001b[0m in \u001b[0;36mmyaverage\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mthesum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot sum things of different types\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthesum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot sum things of different types"
     ]
    }
   ],
   "source": [
    "myaverage(['a',1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymath.py\n"
     ]
    }
   ],
   "source": [
    "%%file mymath.py\n",
    "def myaverage(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> myaverage([1,2])\n",
    "    1.5\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(l)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate mean of length 0 list\")\n",
    "    try:\n",
    "        thesum = sum(l)\n",
    "    except:\n",
    "        raise TypeError(\"Cannot sum things of different types\")\n",
    "    average = thesum/n\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraction(1, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fractions import Fraction\n",
    "myaverage([Fraction(1,3), Fraction(2,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working the `unittest` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_mymath.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath.py\n",
    "from mymath import myaverage\n",
    "import unittest\n",
    "import numbers\n",
    "from fractions import Fraction\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \n",
    "    def test_mymath(self):\n",
    "        self.assertEqual(myaverage([2,3]), 2.5)\n",
    "        \n",
    "    def test_mymath_result(self):\n",
    "        self.assertTrue(isinstance(myaverage([Fraction(1,3), Fraction(2,3)]), numbers.Real))\n",
    "\n",
    "    def test_char(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            myaverage(['a',3])\n",
    "            \n",
    "    def test_zerol(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            myaverage([])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 4 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m unittest test_mymath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A test harness: The `py.test` program\n",
    "\n",
    "As a group you should choose a test harness. I like `py.test`: it will run some `nosetes`s, `unittest`s, as well as its own set. Here we talk about `py.test`, but the principles are the same for any such framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install thus. Make sure you are in the `py35` virtual environment.\n",
    "\n",
    "`pip install pytest`\n",
    "\n",
    "`pip install pytest-cov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 4 items \n",
      "\u001b[0m\n",
      "test_mymath.py ....\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 4 passed in 0.03 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\r\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \r\n",
      "plugins: cov-2.2.1\r\n",
      "\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 1 items\u001b[0m\u001b[1m\r",
      "collecting 1 items\u001b[0m\u001b[1m\r",
      "collecting 5 items\u001b[0m\u001b[1m\r",
      "collecting 5 items\u001b[0m\u001b[1m\r",
      "collected 5 items \r\n",
      "\u001b[0m\r\n",
      "mymath.py .\r\n",
      "test_mymath.py ....\r\n",
      "\r\n",
      "\u001b[1m\u001b[32m=========================== 5 passed in 0.03 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense, it would be nice to somehow check that every line in a program has been covered by a test, so that you know that line has not contributed to making something wrong. But this is hard to do: it would be hard to use normal input data to force a program to go through particular statements. So we settle for testing the important lines. The `pytest-cov` module makes sure that this works.\n",
    "\n",
    "Coverage does not mean that every edge case has been tried, but rather, every critical statement has been.\n",
    "\n",
    "We add a median function to our file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mymedian(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> mymedian([1,2,3])\n",
    "    2\n",
    "    \n",
    "    >>> mymedian([1,2,3,4])\n",
    "    2.5\n",
    "    \"\"\"\n",
    "    lsorted = sorted(l)\n",
    "    mididx = len(lsorted)//2\n",
    "    if len(lsorted) % 2 == 0: #even\n",
    "        return (lsorted[mididx-1] + lsorted[mididx])/2\n",
    "    else:\n",
    "        return lsorted[mididx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymath.py\n"
     ]
    }
   ],
   "source": [
    "%%file mymath.py\n",
    "\n",
    "def myaverage(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> myaverage([1,2])\n",
    "    1.5\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(l)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate mean of length 0 list\")\n",
    "    try:\n",
    "        thesum = sum(l)\n",
    "    except:\n",
    "        raise TypeError(\"Cannot sum things of different types\")\n",
    "    average = thesum/n\n",
    "    return average\n",
    "\n",
    "def mymedian(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> mymedian([1,2,3])\n",
    "    2\n",
    "    \n",
    "    >>> mymedian([1,2,3,4])\n",
    "    2.5\n",
    "    \"\"\"\n",
    "    lsorted = sorted(l)\n",
    "    mididx = len(lsorted)//2\n",
    "    if len(lsorted) % 2 == 0: #even\n",
    "        return (lsorted[mididx-1] + lsorted[mididx])/2\n",
    "    else:\n",
    "        return lsorted[mididx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `py.test` with coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from mymath import myaverage, mymedian\r\n",
      "import unittest\r\n",
      "import numbers\r\n",
      "from fractions import Fraction\r\n",
      "\r\n",
      "class MyTest(unittest.TestCase):\r\n",
      "    \r\n",
      "    def test_mymath(self):\r\n",
      "        self.assertEqual(myaverage([2,3]), 2.5)\r\n",
      "        \r\n",
      "    def test_mymath_result(self):\r\n",
      "        self.assertTrue(isinstance(myaverage([Fraction(1,3), Fraction(2,3)]), numbers.Real))\r\n",
      "\r\n",
      "    def test_char(self):\r\n",
      "        with self.assertRaises(TypeError):\r\n",
      "            myaverage(['a',3])\r\n",
      "            \r\n",
      "    def test_zerol(self):\r\n",
      "        with self.assertRaises(ValueError):\r\n",
      "            myaverage([])\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    unittest.main()"
     ]
    }
   ],
   "source": [
    "!cat test_mymath.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1 -- //anaconda/envs/py35/bin/python\n",
      "cachedir: .cache\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 6 items \n",
      "\u001b[0m\n",
      "mymath.py::mymath.myaverage \u001b[32mPASSED\u001b[0m\n",
      "mymath.py::mymath.mymedian \u001b[32mPASSED\u001b[0m\n",
      "test_mymath.py::MyTest::test_char \u001b[32mPASSED\u001b[0m\n",
      "test_mymath.py::MyTest::test_mymath \u001b[32mPASSED\u001b[0m\n",
      "test_mymath.py::MyTest::test_mymath_result \u001b[32mPASSED\u001b[0m\n",
      "test_mymath.py::MyTest::test_zerol \u001b[32mPASSED\u001b[0m\n",
      "--------------- coverage: platform darwin, python 3.5.1-final-0 ----------------\n",
      "Name             Stmts   Miss  Cover\n",
      "------------------------------------\n",
      "mymath.py           16      0   100%\n",
      "test_mymath.py      17      1    94%\n",
      "------------------------------------\n",
      "TOTAL               33      1    97%\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 6 passed in 0.09 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules --cov --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can ask for a coverage report with missing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 6 items \n",
      "\u001b[0m\n",
      "mymath.py ..\n",
      "test_mymath.py ....\n",
      "--------------- coverage: platform darwin, python 3.5.1-final-0 ----------------\n",
      "Name             Stmts   Miss  Cover   Missing\n",
      "----------------------------------------------\n",
      "mymath.py           16      0   100%   \n",
      "test_mymath.py      17      1    94%   24\n",
      "----------------------------------------------\n",
      "TOTAL               33      1    97%   \n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 6 passed in 0.06 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules --cov --cov-report term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mymedian` has its problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unorderable types: int() < str()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-5382d15ce334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmymedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-32e76a35022b>\u001b[0m in \u001b[0;36mmymedian\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;36m2.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlsorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmididx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#even\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: int() < str()"
     ]
    }
   ],
   "source": [
    "mymedian(['a',1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-3469736f214c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmymedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-32e76a35022b>\u001b[0m in \u001b[0;36mmymedian\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmididx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsorted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#even\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlsorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmididx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlsorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmididx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlsorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmididx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "mymedian([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fix by adressing preconditions in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymath.py\n"
     ]
    }
   ],
   "source": [
    "%%file mymath.py\n",
    "\n",
    "def myaverage(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> myaverage([1,2])\n",
    "    1.5\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(l)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate mean of length 0 list\")\n",
    "    try:\n",
    "        thesum = sum(l)\n",
    "    except:\n",
    "        raise TypeError(\"Cannot sum things of different types\")\n",
    "    average = thesum/n\n",
    "    return average\n",
    "\n",
    "def mymedian(l:list)->float:\n",
    "    \"\"\"\n",
    "    Calculate the average of list l\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    >>> mymedian([1,2,3])\n",
    "    2\n",
    "    \n",
    "    >>> mymedian([1,2,3,4])\n",
    "    2.5\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lsorted = sorted(l)\n",
    "    except:\n",
    "        raise TypeError(\"Unable to sort array\")\n",
    "    n = len(lsorted)\n",
    "    if n==0:\n",
    "        raise ValueError(\"cant calculate median of length 0 list\")\n",
    "    mididx = len(lsorted)//2\n",
    "    if len(lsorted) % 2 == 0: #even\n",
    "        return (lsorted[mididx-1] + lsorted[mididx])/2\n",
    "    else:\n",
    "        return lsorted[mididx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Practice:\n",
    "\n",
    ">The  effort  of  testing as  you  go  is  minimal  and  pays off  handsomely.  Thinking about testing as you  write a program will  lead to better code, because that's when you know  best  what the code should do.  If  instead  you  wait  until  something breaks, you will  probably  have forgotten how  the code works.  Working under  pressure, you  will need  to figure it  out again, which  takes time, and  the fixes  will  be  less  thorough  and more fragile because your refreshed understanding is  likely to be incomplete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to py.test\n",
    "\n",
    "- pytest runs doctests and unittests.\n",
    "- any function prefixed with `test_` is a test.\n",
    "- will tey nosetests\n",
    "\n",
    "(you dont have to use py.test, and can use nosetests if your team prefers it)\n",
    "\n",
    "You can continue to use `unittest`. `py.test` has the advantage that simple asserts are transformed the way stuff like `assertEqual` works. Not only that, `py.test` gives you some useful message as to why the asertion failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_mymath2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath2.py\n",
    "\n",
    "from pytest import raises\n",
    "from mymath import myaverage, mymedian\n",
    "\n",
    "def test_mymath_mean():\n",
    "    assert myaverage([9,3]) == 6\n",
    "\n",
    "def test_char():\n",
    "    with raises(TypeError):\n",
    "        myaverage(['a',3])\n",
    "\n",
    "def test_mymath():\n",
    "    assert mymedian([9,3, 6]) == 5\n",
    "    \n",
    "def test_zero_median():\n",
    "    with raises(ValueError):\n",
    "        mymedian([])\n",
    "        \n",
    "def test_char_median():\n",
    "    with raises(TypeError):\n",
    "        mymedian(['a', 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Know the answer or get it simply otherwise\n",
    "\n",
    "Be very careful that your test suite is correct. The above is not, and whether the test suite is wrong, or our functions are not doing the right thing, `py.test` gives a useful message. If you need to calculate an answer, use a simpler method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 11 items \n",
      "\u001b[0m\n",
      "mymath.py ..\n",
      "test_mymath.py ....\n",
      "test_mymath2.py ..F..\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_mymath __________________________________\n",
      "\n",
      "\u001b[1m    def test_mymath():\u001b[0m\n",
      "\u001b[1m>       assert mymedian([9,3, 6]) == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 6 == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 6 = mymedian([9, 3, 6])\u001b[0m\n",
      "\n",
      "test_mymath2.py:13: AssertionError\n",
      "--------------- coverage: platform darwin, python 3.5.1-final-0 ----------------\n",
      "Name              Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------------\n",
      "mymath.py            22      0   100%   \n",
      "test_mymath.py       17      1    94%   24\n",
      "test_mymath2.py      15      0   100%   \n",
      "-----------------------------------------------\n",
      "TOTAL                54      1    98%   \n",
      "\u001b[1m\u001b[31m===================== 1 failed, 10 passed in 0.10 seconds ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules --cov --cov-report term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_mymath2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath2.py\n",
    "\n",
    "from pytest import raises\n",
    "from mymath import myaverage, mymedian\n",
    "\n",
    "def test_mymath_mean():\n",
    "    assert myaverage([9,3]) == 6\n",
    "\n",
    "def test_char():\n",
    "    with raises(TypeError):\n",
    "        myaverage(['a',3])\n",
    "\n",
    "def test_mymath():\n",
    "    assert mymedian([9,3, 6]) == 6\n",
    "    \n",
    "def test_zero_median():\n",
    "    with raises(ValueError):\n",
    "        mymedian([])\n",
    "        \n",
    "def test_char_median():\n",
    "    with raises(TypeError):\n",
    "        mymedian(['a', 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\r\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \r\n",
      "plugins: cov-2.2.1\r\n",
      "\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 2 items\u001b[0m\u001b[1m\r",
      "collecting 2 items\u001b[0m\u001b[1m\r",
      "collecting 6 items\u001b[0m\u001b[1m\r",
      "collecting 6 items\u001b[0m\u001b[1m\r",
      "collecting 6 items\u001b[0m\u001b[1m\r",
      "collecting 11 items\u001b[0m\u001b[1m\r",
      "collected 11 items \r\n",
      "\u001b[0m\r\n",
      "mymath.py ..\r\n",
      "test_mymath.py ....\r\n",
      "test_mymath2.py .....\r\n",
      "--------------- coverage: platform darwin, python 3.5.1-final-0 ----------------\r\n",
      "Name              Stmts   Miss  Cover   Missing\r\n",
      "-----------------------------------------------\r\n",
      "mymath.py            22      0   100%   \r\n",
      "test_mymath.py       17      1    94%   24\r\n",
      "test_mymath2.py      15      0   100%   \r\n",
      "-----------------------------------------------\r\n",
      "TOTAL                54      1    98%   \r\n",
      "\r\n",
      "\u001b[32m\u001b[1m========================== 11 passed in 0.05 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules --cov --cov-report term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDD\n",
    "\n",
    "From Younker's AgilePython Development:\n",
    "\n",
    ">TDD uses very small development cycles. Tests aren’t written for entire functions. They are written incrementally as the functions are composed. If the chunks get too large, a test- driven developer can always back down to a smaller chunk.\n",
    "The cycles have a distinct four-part rhythm. A test is written, and then it is executed to verify that it fails. A test that succeeds at this point tells you nothing about your new code. (Every day I encounter one that works when I don’t expect it to.) After the test fails, the associ- ated code is written, and then the test is run again. This time it should pass. If it passes, then the process begins anew.\n",
    "\n",
    "One advantage of this is that focusses you on the interface of your function/class, etc. The downside is that it precludes exploration and exploration tests, where you might try different interfaces. Another danger is write you might write tons of small units.\n",
    "\n",
    "Should you do it? TDD is certainly useful at times, but make sure you do it when you have a concrete idea where you are going..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures: setup common scaffolding for your tests\n",
    "\n",
    "Many times you need a common setup and teardown for multiple tests. You might need to populate some data, for example. This is done via fixtures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_mymath3.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath3.py\n",
    "\n",
    "\n",
    "from pytest import fixture\n",
    "from mymath import myaverage, mymedian\n",
    "\n",
    "\n",
    "@fixture\n",
    "def input_data():\n",
    "    return dict(b=[4,5,6], a=['a', 1,2])\n",
    "\n",
    "def test_with_fixture(input_data):\n",
    "    assert myaverage(input_data['b']) == 5\n",
    "    assert mymedian(input_data['b']) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 19 items \n",
      "\u001b[0m\n",
      "amath.py ..\n",
      "mymath.py ..\n",
      "test_amath.py .....\n",
      "test_mymath.py ....\n",
      "test_mymath2.py .....\n",
      "test_mymath3.py .\n",
      "--------------- coverage: platform darwin, python 3.5.1-final-0 ----------------\n",
      "Name              Stmts   Miss  Cover   Missing\n",
      "-----------------------------------------------\n",
      "amath.py             22      1    95%   14\n",
      "mymath.py            22      0   100%   \n",
      "test_amath.py        15      0   100%   \n",
      "test_mymath.py       17      1    94%   24\n",
      "test_mymath2.py      15      0   100%   \n",
      "test_mymath3.py       7      0   100%   \n",
      "-----------------------------------------------\n",
      "TOTAL                98      2    98%   \n",
      "\n",
      "\u001b[1m\u001b[32m========================== 19 passed in 0.09 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules --cov --cov-report term-missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_mymath4.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath4.py\n",
    "\n",
    "\n",
    "from pytest import fixture\n",
    "from mymath import mymedian, myaverage\n",
    "\n",
    "\n",
    "@fixture(scope=\"module\")\n",
    "def input_data():\n",
    "    return dict(b=range(1000))\n",
    "\n",
    "def test_first(input_data):\n",
    "    assert mymedian(input_data['b'])  == 499.5\n",
    "\n",
    "def test_second(input_data):\n",
    "    assert myaverage(input_data['b']) == 499.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\r\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \r\n",
      "plugins: cov-2.2.1\r\n",
      "\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 2 items\u001b[0m\u001b[1m\r",
      "collected 2 items \r\n",
      "\u001b[0m\r\n",
      "test_mymath4.py ..\r\n",
      "\r\n",
      "\u001b[1m\u001b[32m=========================== 2 passed in 0.02 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules test_mymath4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_mymath5.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath5.py\n",
    "from pytest import fixture\n",
    "import io\n",
    "#from docs\n",
    "@fixture\n",
    "def file_data(request): # The fixture MUST have a 'request' argument\n",
    "    text = open(\"test_mymath5.py\")\n",
    "\n",
    "    @request.addfinalizer\n",
    "    def teardown():\n",
    "        text.close()\n",
    "    return text\n",
    "\n",
    "def test_data_type(file_data):\n",
    "    assert isinstance(file_data, io.TextIOWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\r\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \r\n",
      "plugins: cov-2.2.1\r\n",
      "\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 1 items\u001b[0m\u001b[1m\r",
      "collected 1 items \r\n",
      "\u001b[0m\r\n",
      "test_mymath5.py .\r\n",
      "\r\n",
      "\u001b[1m\u001b[32m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules test_mymath5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use fixtures in `unittest` as well..but they look a little bit different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_mymath6.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_mymath6.py\n",
    "\n",
    "from mymath import myaverage, mymedian\n",
    "\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.b = range(1000)\n",
    "        \n",
    "    def tearDown(self):\n",
    "        del self.b\n",
    "        \n",
    "    def test_mymath(self):\n",
    "        self.assertEqual(myaverage(self.b), 499.5)\n",
    "        \n",
    "    def test_char(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            myaverage(['a',3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\r\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \r\n",
      "plugins: cov-2.2.1\r\n",
      "\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collecting 2 items\u001b[0m\u001b[1m\r",
      "collecting 4 items\u001b[0m\u001b[1m\r",
      "collecting 4 items\u001b[0m\u001b[1m\r",
      "collecting 9 items\u001b[0m\u001b[1m\r",
      "collecting 9 items\u001b[0m\u001b[1m\r",
      "collecting 13 items\u001b[0m\u001b[1m\r",
      "collecting 13 items\u001b[0m\u001b[1m\r",
      "collecting 13 items\u001b[0m\u001b[1m\r",
      "collecting 18 items\u001b[0m\u001b[1m\r",
      "collecting 18 items\u001b[0m\u001b[1m\r",
      "collecting 19 items\u001b[0m\u001b[1m\r",
      "collecting 19 items\u001b[0m\u001b[1m\r",
      "collecting 21 items\u001b[0m\u001b[1m\r",
      "collecting 21 items\u001b[0m\u001b[1m\r",
      "collecting 22 items\u001b[0m\u001b[1m\r",
      "collecting 22 items\u001b[0m\u001b[1m\r",
      "collecting 24 items\u001b[0m\u001b[1m\r",
      "collecting 24 items\u001b[0m\u001b[1m\r",
      "collected 24 items \r\n",
      "\u001b[0m\r\n",
      "amath.py ..\r\n",
      "mymath.py ..\r\n",
      "test_amath.py .....\r\n",
      "test_mymath.py ....\r\n",
      "test_mymath2.py .....\r\n",
      "test_mymath3.py .\r\n",
      "test_mymath4.py ..\r\n",
      "test_mymath5.py .\r\n",
      "test_mymath6.py ..\r\n",
      "--------------- coverage: platform darwin, python 3.5.1-final-0 ----------------\r\n",
      "Name              Stmts   Miss  Cover   Missing\r\n",
      "-----------------------------------------------\r\n",
      "amath.py             22      1    95%   14\r\n",
      "mymath.py            22      0   100%   \r\n",
      "test_amath.py        15      0   100%   \r\n",
      "test_mymath.py       17      1    94%   24\r\n",
      "test_mymath2.py      15      0   100%   \r\n",
      "test_mymath3.py       7      0   100%   \r\n",
      "test_mymath4.py       8      0   100%   \r\n",
      "test_mymath5.py       9      0   100%   \r\n",
      "test_mymath6.py      12      0   100%   \r\n",
      "-----------------------------------------------\r\n",
      "TOTAL               127      2    98%   \r\n",
      "\r\n",
      "\u001b[32m\u001b[1m========================== 24 passed in 0.41 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --doctest-modules --cov --cov-report term-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fakes and Mocks: make your tests self-contained\n",
    "\n",
    "But in general you shouldnt even be dealing with a file or db. u should **mock** it out. Your tests should nor rely on a database being there, or a network connection being possible. Otherwise, you dont know where your failure came from.\n",
    "\n",
    "This is not to saythat you shouldnt test against a real network or real database. This is indeed the domain of acceptance tests. But unit tests where you are testing a layer or just a small unit should be isolated and not concern itself outside the unit.\n",
    "\n",
    "Typically i tend to \"test\" against the real deal, and then write a fake which simulates it for formal tests. You can see the procedure below for a mail client.\n",
    "\n",
    "(But first, you can set up fixtures in a `conftest.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%file conftest.py\n",
    "import pytest\n",
    "import smtplib\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def smtp():\n",
    "    return smtplib.SMTP(\"smtp.gmail.com\", 587)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what really happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,\n",
       " b'smtp.gmail.com at your service, [50.177.146.107]\\nSIZE 35882577\\n8BITMIME\\nSTARTTLS\\nENHANCEDSTATUSCODES\\nPIPELINING\\nCHUNKING\\nSMTPUTF8')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import smtplib\n",
    "xxx=smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "xxx.ehlo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, b'2.0.0 OK g6sm5963317qgd.5 - gsmtp')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx.noop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_smtp.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_smtp.py\n",
    "\n",
    "def test_ehlo(smtp):\n",
    "    response, msg = smtp.ehlo()\n",
    "    assert response == 250\n",
    "    assert b\"smtp.gmail.com\" in msg\n",
    "\n",
    "def test_noop(smtp):\n",
    "    response, msg = smtp.noop()\n",
    "    assert response == 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 2 items \n",
      "\u001b[0m\n",
      "test_smtp.py ..\n",
      "\n",
      "\u001b[1m\u001b[32m=========================== 2 passed in 0.18 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_smtp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but we shouldnt be stuck if the smtp server is down....so we look at the output from the previous and \"fake\" it for the `ehlo` and `noop` ops.\n",
    "\n",
    "Notice below that its critical to \"mock\" the original object by momkey-patching in the `ehlo` and `noop` methods. The `monkeypatch` object in `py.test` will handle setting the original object's methods to the new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%file conftest.py\n",
    "import pytest\n",
    "import smtplib\n",
    "\n",
    "def ehlo(smtpi):\n",
    "    return (250, b'smtp.gmail.com BLA BLA')\n",
    "def noop(smtpi):\n",
    "    return (250, b'BLA BLA gsmtp')\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def smtp(monkeypatch):\n",
    "    #smtp_instance = smtplib.SMTP()\n",
    "    monkeypatch.setattr(smtplib.SMTP, \"ehlo\", ehlo)\n",
    "    monkeypatch.setattr(smtplib.SMTP, \"noop\", noop)\n",
    "    return smtplib.SMTP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 2 items \n",
      "\u001b[0m\n",
      "test_smtp.py ..\n",
      "\n",
      "\u001b[32m\u001b[1m=========================== 2 passed in 4.48 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_smtp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done by using `mock` and `unittest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_smtp2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_smtp2.py\n",
    "\n",
    "import unittest\n",
    "import unittest.mock as mock\n",
    "import smtplib\n",
    "\n",
    "def ehlo():\n",
    "    return (250, b'smtp.gmail.com BLA BLA')\n",
    "def noop():\n",
    "    return (250, b'BLA BLA gsmtp')\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.patcher = mock.patch(\"smtplib.SMTP\")\n",
    "        self.smtp = self.patcher.start()\n",
    "        self.smtp.ehlo=ehlo\n",
    "        self.smtp.noop=noop\n",
    "        \n",
    "    def tearDown(self):\n",
    "        self.patcher.stop()\n",
    "\n",
    "    def test_ehlo(self):\n",
    "        response, msg = self.smtp.ehlo()\n",
    "        assert response == 250\n",
    "        assert b\"smtp.gmail.com\" in msg\n",
    "\n",
    "    def test_noop(self):\n",
    "        response, msg = self.smtp.noop()\n",
    "        assert response == 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.5.1, pytest-2.8.1, py-1.4.30, pluggy-0.3.1\n",
      "rootdir: /Users/rahul/Projects/private/cs207/lecswithlabs/week5, inifile: \n",
      "plugins: cov-2.2.1\n",
      "collected 2 items \n",
      "\u001b[0m\n",
      "test_smtp2.py ..\n",
      "\n",
      "\u001b[1m\u001b[32m=========================== 2 passed in 3.16 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_smtp2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your project, you will want to mock out your database connections in your unit tests. There are other libraries you can use as well, such as `minimock`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
